{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGlue Demo\n",
    "In this notebook we match two pairs of images using LightGlue with early stopping and point pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGlue\n"
     ]
    }
   ],
   "source": [
    "# If we are on colab: this clones the repo and installs the dependencies\n",
    "from pathlib import Path\n",
    "\n",
    "if Path.cwd().name != \"LightGlue\":\n",
    "    !git clone --quiet https://github.com/cvg/LightGlue/\n",
    "    %cd LightGlue\n",
    "    !pip install --progress-bar off --quiet -e .\n",
    "\n",
    "from lightglue import LightGlue, SuperPoint, DISK\n",
    "from lightglue.utils import load_image, rbd\n",
    "from lightglue import viz2d\n",
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "images = Path(\"assets\")\n",
    "print(Path.cwd().name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findParticlesKeypointDescriptors(particlesPxPos,yaw = 0, snapDim = (256,256), keypointBase_np = None, featuresBase = None, detector = 'SP', device = 'cuda'):\n",
    "    \"\"\"\n",
    "    Filters ORB keypoints and descriptors that lie within a rotated rectangle.\n",
    "    \n",
    "    Parameters:\n",
    "    - keypoints: List of cv2.KeyPoint objects.\n",
    "    - descriptors: numpy array of shape (N, D), corresponding descriptors.\n",
    "    - rect_center: tuple (x_c, y_c), center of the rectangle.\n",
    "    - rect_size: tuple (w, h), dimensions of the rectangle (width, height).\n",
    "    - angle: float, rotation angle of the rectangle in degrees (counterclockwise).\n",
    "\n",
    "    Returns:\n",
    "    - filtered_keypoints: List of cv2.KeyPoint objects inside the rectangle.\n",
    "    - filtered_descriptors: numpy array of descriptors corresponding to those keypoints.\n",
    "    \"\"\"\n",
    "    # Convert cv2.KeyPoint objects to NumPy array of coordinates\n",
    "    \n",
    "    # Convert from NED world frame to px(u,v)\n",
    "    # particlesWorldPos NX2 array\n",
    "    \n",
    "    w, h = snapDim\n",
    "\n",
    "    # N = particlesPxPos.shape[0]\n",
    "\n",
    "    # Find min-max x,y in particles\n",
    "    min_x = particlesPxPos[0].min() - (w // 2)\n",
    "    max_x = particlesPxPos[0].max() + (w // 2)\n",
    "    min_y = particlesPxPos[1].min() - (h // 2)\n",
    "    max_y = particlesPxPos[1].max() + (h // 2)\n",
    "    \n",
    "    reduced_mask = (\n",
    "                    (keypointBase_np[:, 0] <= max_x) & (keypointBase_np[:, 0] >= min_x) &\n",
    "                    (keypointBase_np[:, 1] <= max_y) & (keypointBase_np[:, 1] >= min_y)\n",
    "                    )\n",
    "    \n",
    "    # aug_keypoints_np = [kp for kp, mask in zip(keypoints_np, reduced_mask) if mask]\n",
    "    reduced_keypoints = keypointBase_np[reduced_mask]\n",
    "    if detector == 'ORB':\n",
    "        reduced_descriptors = featuresBase[reduced_mask]\n",
    "    elif detector == 'SP':\n",
    "        keypoints, keypoint_scores, descriptors, image_size = featuresBase[\"keypoints\"][:,reduced_mask,:] , featuresBase[\"keypoint_scores\"][:,reduced_mask], featuresBase[\"descriptors\"][:,reduced_mask,:] , featuresBase[\"image_size\"]\n",
    "        image_size = torch.from_numpy(np.array([w,h])[np.newaxis, : ]).to(device)\n",
    "        # reduced_descriptors = {\"keypoints\" : keypoints, \"keypoint_scores\" : keypoint_scores, \"descriptors\" : descriptors , \"image_size\" : image_size}\n",
    "        \n",
    "        # scales, oris =  featuresBase[\"scales\"][:,reduced_mask] , featuresBase[\"oris\"][:,reduced_mask]\n",
    "        # keypoints = keypoints - torch.from_numpy(np.array([min_x , min_y])[np.newaxis, : ]).to(device)\n",
    "        reduced_descriptors = {\"keypoints\" : keypoints, \"keypoint_scores\" : keypoint_scores, \"descriptors\" : descriptors , \"image_size\" : image_size}#, \"scales\" : scales, \"oris\" : oris}\n",
    "        \n",
    "        \n",
    "    # ParticlesKeypoints      = []\n",
    "    # ParticlesDescriptors    = []\n",
    "    # ParticlesLocalKeypoints = []\n",
    "\n",
    "    # with Timer('dd'):\n",
    "    # for i in range(N):\n",
    "    # yaw = particlesYaw[i]  # rad\n",
    "    # Compute rotation matrix\n",
    "    R = np.array([\n",
    "        [ np.cos(yaw),  np.sin(yaw)],\n",
    "        [-np.sin(yaw),  np.cos(yaw)]\n",
    "    ])\n",
    "\n",
    "    # Shift keypoints to rectangle's center\n",
    "    shifted_keypoints = reduced_keypoints - particlesPxPos\n",
    "\n",
    "    # Rotate keypoints to rectangle's local frame\n",
    "    local_keypoints = np.dot(shifted_keypoints, R) \n",
    "            \n",
    "    inside_mask = (\n",
    "        (np.abs(local_keypoints[:, 0]) <= w // 2) &\n",
    "        (np.abs(local_keypoints[:, 1]) <= h // 2)\n",
    "    )\n",
    "    \n",
    "    # Mask inside features\n",
    "    particle_keypoint        = reduced_keypoints[inside_mask]\n",
    "    particle_local_keyppoint = local_keypoints[inside_mask]  + np.array([ w // 2, h // 2])    # Particles Local Keypoints (relative keypoints  to uppler left corner of particles px)\n",
    "    if detector == 'ORB':\n",
    "        particle_descriptor = reduced_descriptors[inside_mask]\n",
    "    elif detector == 'SP':\n",
    "        keypoints, keypoint_scores, descriptors, image_size = reduced_descriptors[\"keypoints\"][:,inside_mask,:] , reduced_descriptors[\"keypoint_scores\"][:,inside_mask], reduced_descriptors[\"descriptors\"][:,inside_mask,:] , reduced_descriptors[\"image_size\"]\n",
    "        print(w,h)\n",
    "        image_size = torch.from_numpy(np.array([w,h])[np.newaxis, : ]).to(device)\n",
    "        # particle_descriptor = {\"keypoints\" : keypoints, \"keypoint_scores\" : keypoint_scores, \"descriptors\" : descriptors , \"image_size\" : image_size}\n",
    "\n",
    "        # scales, oris =  reduced_descriptors[\"scales\"][:,inside_mask] , reduced_descriptors[\"oris\"][:,inside_mask]\n",
    "        particle_descriptor = {\"keypoints\" : keypoints, \"keypoint_scores\" : keypoint_scores, \"descriptors\" : descriptors , \"image_size\" : image_size}#, \"scales\" : scales, \"oris\" : oris}\n",
    "        \n",
    "    # ParticlesKeypoints.append(particle_keypoint)\n",
    "    # ParticlesDescriptors.append(particle_descriptor)\n",
    "    # ParticlesLocalKeypoints.append(particle_local_keyppoint)\n",
    "                \n",
    "    return particle_local_keyppoint,particle_keypoint,particle_descriptor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load extractor and matcher module\n",
    "In this example we use SuperPoint features combined with LightGlue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lightglue import LightGlue, SIFT, ALIKED\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 'mps', 'cpu'\n",
    "\n",
    "# extractor = SuperPoint(max_num_keypoints=None).eval().to(device)  # load the extractor\n",
    "extractor = SuperPoint(max_num_keypoints=2048, detection_threshold=0.0).eval().to(device)  # load the extractor\n",
    "match_conf = {\n",
    "    'width_confidence': -1,  # for point pruning\n",
    "    'depth_confidence': -1,  # for early stopping,\n",
    "    'flash': False,\n",
    "}\n",
    "# matcher = matcher = LightGlue(features='superpoint', depth_confidence=0.9, width_confidence=0.95).eval().cuda()\n",
    "matcher = LightGlue(features='superpoint', **match_conf).eval().to(device)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# from utils import drawKeypoints\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy example\n",
    "The top image shows the matches, while the bottom image shows the point pruning across layers. In this case, LightGlue prunes a few points with occlusions, but is able to stop the context aggregation after 4/9 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048, 2])\n",
      "220 220\n",
      "torch.Size([1, 1194, 2])\n",
      "231\n"
     ]
    }
   ],
   "source": [
    "# image0 = load_image(\"C:/Users/cyber/OneDrive/Desktop/bakisanlan/github_repos/FeatureMatchingPYTHON/data/itu_map_square.jpg\")\n",
    "# image1 = load_image(\"C:/Users/cyber/OneDrive/Desktop/bakisanlan/github_repos/FeatureMatchingPYTHON/data/itu_map_square.jpg\")\n",
    "\n",
    "# feats0 = extractor.extract(image0.to(device))\n",
    "# print(feats0[\"scales\"].shape)\n",
    "# feats1 = extractor.extract(image1.to(device))\n",
    "\n",
    "\n",
    "# image0 = load_image('C:/Users/cyber/OneDrive/Desktop/bakisanlan/github_repos/FeatureMatchingPYTHON/data/cyclegan/turbo/winterUAV2summerSAT_test/real_sat_asvideo/sat_1800.jpg')\n",
    "image0 = load_image('C:/Users/cyber/OneDrive/Desktop/bakisanlan/github_repos/FeatureMatchingPYTHON/data/cyclegan/turbo/winterUAV2summerSAT_test/v2/fid-16251/samples_a2b/103.png')\n",
    "# image0 = load_image('C:/Users/cyber/OneDrive/Desktop/bakisanlan/github_repos/FeatureMatchingPYTHON/data/cyclegan/turbo/winterUAV2summerSAT_test/v1/fid_reference_b2a/frame_2160.png')\n",
    "# image0 = load_image('C:/Users/cyber/OneDrive/Desktop/bakisanlan/github_repos/FeatureMatchingPYTHON/data/cyclegan/turbo/winterUAV2summerSAT_test/v1/fid_reference_b2a/frame_1800.png')\n",
    "\n",
    "\n",
    "feats0_org = extractor.extract(image0.to(device))\n",
    "print(feats0_org[\"keypoints\"].shape)\n",
    "\n",
    "\n",
    "image1 = load_image(\"C:/Users/cyber/OneDrive/Desktop/bakisanlan/github_repos/FeatureMatchingJUSTCODE/FeatureMatching-PythonCODE/data/itu_sat.jpg\")\n",
    "\n",
    "with open('C:/Users/cyber/OneDrive/Desktop/bakisanlan/github_repos/FeatureMatchingJUSTCODE/FeatureMatching-PythonCODE/data/feature_map/SP/2048/descriptors.pkl', 'rb') as f:\n",
    "    feats_base = pickle.load(f)  \n",
    "with open('C:/Users/cyber/OneDrive/Desktop/bakisanlan/github_repos/FeatureMatchingJUSTCODE/FeatureMatching-PythonCODE/data/feature_map/SP/2048/keypoints_np.pkl', 'rb') as f:\n",
    "    keypointBase_np = pickle.load(f)  \n",
    "\n",
    "particle_local_keyppoint,particle_keypoint,feats1_org = findParticlesKeypointDescriptors(np.array([2848+128,3450+128]),yaw = 0, snapDim = (220,220), keypointBase_np = keypointBase_np, featuresBase = feats_base)\n",
    "print(feats1_org[\"keypoints\"].shape)\n",
    "# feats1_org[\"image_size\"] = torch.from_numpy(np.array([200,200])[np.newaxis, : ]).to(device)\n",
    "\n",
    "matches01 = matcher({\"image0\": feats0_org, \"image1\": feats1_org})\n",
    "# print(feats0[\"keypoints\"].shape)\n",
    "feats0, feats1, matches01 = [\n",
    "    rbd(x) for x in [feats0_org, feats1_org, matches01]\n",
    "]  # remove batch dimension\n",
    "# print(image0.shape)\n",
    "\n",
    "kpts0, kpts1, matches = feats0[\"keypoints\"], feats1[\"keypoints\"], matches01[\"matches\"]\n",
    "m_kpts0, m_kpts1 = kpts0[matches[..., 0]], kpts1[matches[..., 1]]\n",
    "\n",
    "\n",
    "# axes = viz2d.plot_images([image0, image1])\n",
    "# viz2d.plot_matches(m_kpts0, m_kpts1, color=\"lime\", lw=0.2)\n",
    "# viz2d.add_text(0, f'Stop after {matches01[\"stop\"]} layers', fs=20)\n",
    "\n",
    "# kpc0, kpc1 = viz2d.cm_prune(matches01[\"prune0\"]), viz2d.cm_prune(matches01[\"prune1\"])\n",
    "# viz2d.plot_images([image0, image1])\n",
    "# viz2d.plot_keypoints([kpts0, kpts1], colors=[kpc0, kpc1], ps=10)\n",
    "print(len(m_kpts0))\n",
    "# print(m_kpts1)\n",
    "\n",
    "\n",
    "\n",
    "# n = 50\n",
    "# for key in feats0.keys():\n",
    "#     feats0[key] = torch.cat([feats0[key]] * n)\n",
    "#     feats1[key] = torch.cat([feats1[key]] * n)\n",
    "\n",
    "# for i in range(10):\n",
    "#     last_time=time()\n",
    "#     with torch.inference_mode():\n",
    "#         pred1 = matcher({'image0': feats0, 'image1': feats1})\n",
    "#     current_time=time()\n",
    "#     print(current_time-last_time)    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2871.3750, 3531.8750],\n",
       "         [3002.8750, 3551.3750],\n",
       "         [3039.6250, 3511.3750],\n",
       "         ...,\n",
       "         [3076.3750, 3585.8750],\n",
       "         [3085.1250, 3646.3750],\n",
       "         [3079.6250, 3639.3750]]], device='cuda:0')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats1_org[\"keypoints\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = [[] for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(219.875)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats1_org[\"keypoints\"].cpu().numpy()[0][:,1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "854\n",
      "torch.Size([1, 2048, 256])\n"
     ]
    }
   ],
   "source": [
    "n_pad = abs(feats0[\"keypoints\"].shape[0] - feats1[\"keypoints\"].shape[0])\n",
    "print(n_pad)\n",
    "\n",
    "feat_willbe_padded = feats0_org.copy()\n",
    "if feats0_org[\"keypoints\"].shape[1] > feats1_org[\"keypoints\"].shape[1]:\n",
    "    feat_willbe_padded = feats1_org.copy()\n",
    "    \n",
    "\n",
    "# pad_kp    =  torch.ones((feat_willbe_padded[\"keypoints\"].shape[0], n_pad, 2)).to(feat_willbe_padded[\"keypoints\"].device)\n",
    "pad_kp    =  -1*torch.ones((feat_willbe_padded[\"keypoints\"].shape[0], n_pad, 2)).to(feat_willbe_padded[\"keypoints\"].device)\n",
    "# pad_desc  =  -torch.full((feat_willbe_padded[\"descriptors\"].shape[0], n_pad, feat_willbe_padded[\"descriptors\"].shape[2]),float('inf')).to(feat_willbe_padded[\"descriptors\"].device)\n",
    "pad_desc  =  -torch.ones((feat_willbe_padded[\"descriptors\"].shape[0], n_pad, feat_willbe_padded[\"descriptors\"].shape[2])).to(feat_willbe_padded[\"descriptors\"].device) / 256\n",
    "pad_score = torch.full((feat_willbe_padded[\"keypoint_scores\"].shape[0], n_pad),float('inf')).to(feat_willbe_padded[\"keypoint_scores\"].device)\n",
    "\n",
    "feat_willbe_padded[\"keypoints\"]       = torch.cat([feat_willbe_padded[\"keypoints\"], pad_kp], dim=1)\n",
    "feat_willbe_padded[\"descriptors\"]     = torch.cat([feat_willbe_padded[\"descriptors\"], pad_desc], dim=1) \n",
    "feat_willbe_padded[\"keypoint_scores\"] = torch.cat([feat_willbe_padded[\"keypoint_scores\"], pad_score], dim=1)\n",
    "\n",
    "\n",
    "print(feat_willbe_padded[\"descriptors\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
       "         [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
       "         [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
       "         ...,\n",
       "         [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
       "         [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
       "         [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(feats0_org[\"keypoints\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "matches01_2 = matcher({\"image0\": feats0_org, \"image1\": feat_willbe_padded})\n",
    "# print(feats0[\"keypoints\"].shape)\n",
    "feats0_fix, feats1_fix, matches01_2 = [\n",
    "    rbd(x) for x in [feats0_org, feat_willbe_padded, matches01_2]\n",
    "]  # remove batch dimension\n",
    "# print(image0.shape)\n",
    "\n",
    "kpts0, kpts1, matches_2 = feats0_fix[\"keypoints\"], feats1_fix[\"keypoints\"], matches01_2[\"matches\"]\n",
    "m_kpts0, m_kpts1 = kpts0[matches_2[..., 0]], kpts1[matches_2[..., 1]]\n",
    "\n",
    "\n",
    "# axes = viz2d.plot_images([image0, image1])\n",
    "# viz2d.plot_matches(m_kpts0, m_kpts1, color=\"lime\", lw=0.2)\n",
    "# viz2d.add_text(0, f'Stop after {matches01[\"stop\"]} layers', fs=20)\n",
    "\n",
    "# kpc0, kpc1 = viz2d.cm_prune(matches01[\"prune0\"]), viz2d.cm_prune(matches01[\"prune1\"])\n",
    "# viz2d.plot_images([image0, image1])\n",
    "# viz2d.plot_keypoints([kpts0, kpts1], colors=[kpc0, kpc1], ps=10)\n",
    "# print(len(m_kpts0.cpu().numpy()))\n",
    "\n",
    "print(len(m_kpts0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches0': tensor([-1, -1, -1,  ..., -1, -1, -1], device='cuda:0'),\n",
       " 'matches1': tensor([-1, -1, -1,  ..., -1, -1, -1], device='cuda:0'),\n",
       " 'matching_scores0': tensor([0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
       " 'matching_scores1': tensor([0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
       " 'stop': 9,\n",
       " 'matches': tensor([[286, 865],\n",
       "         [487, 757]], device='cuda:0'),\n",
       " 'scores': tensor([0.1205, 0.1021], device='cuda:0'),\n",
       " 'prune0': tensor([9., 9., 9.,  ..., 9., 9., 9.], device='cuda:0'),\n",
       " 'prune1': tensor([9., 9., 9.,  ..., 9., 9., 9.], device='cuda:0')}"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches01_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(m_kpts0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(229, 2)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = matches.cpu().numpy()\n",
    "a1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2 = matches_2.cpu().numpy()\n",
    "a2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(231, 2)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1_with_row = np.vstack([a1, np.zeros((2,2),dtype=int)])\n",
    "a1_with_row.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[286, 865]], device='cuda:0')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 231 and the array at index 1 has size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma1_with_row\u001b[49m\u001b[43m,\u001b[49m\u001b[43ma2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \n",
      "File \u001b[1;32mc:\\Users\\cyber\\anaconda3\\envs\\bakiMPF\\Lib\\site-packages\\numpy\\_core\\shape_base.py:367\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 231 and the array at index 1 has size 1"
     ]
    }
   ],
   "source": [
    "np.hstack([a1_with_row,a2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findParticlesKeypointDescriptors(particlesPxPos,yaw = 0, snapDim = (256,256), keypointBase_np = None, featuresBase = None, detector = 'SP', device = 'cuda'):\n",
    "    \"\"\"\n",
    "    Filters ORB keypoints and descriptors that lie within a rotated rectangle.\n",
    "    \n",
    "    Parameters:\n",
    "    - keypoints: List of cv2.KeyPoint objects.\n",
    "    - descriptors: numpy array of shape (N, D), corresponding descriptors.\n",
    "    - rect_center: tuple (x_c, y_c), center of the rectangle.\n",
    "    - rect_size: tuple (w, h), dimensions of the rectangle (width, height).\n",
    "    - angle: float, rotation angle of the rectangle in degrees (counterclockwise).\n",
    "\n",
    "    Returns:\n",
    "    - filtered_keypoints: List of cv2.KeyPoint objects inside the rectangle.\n",
    "    - filtered_descriptors: numpy array of descriptors corresponding to those keypoints.\n",
    "    \"\"\"\n",
    "    # Convert cv2.KeyPoint objects to NumPy array of coordinates\n",
    "    \n",
    "    # Convert from NED world frame to px(u,v)\n",
    "    # particlesWorldPos NX2 array\n",
    "    \n",
    "    w, h = snapDim\n",
    "\n",
    "    # N = particlesPxPos.shape[0]\n",
    "\n",
    "    # Find min-max x,y in particles\n",
    "    min_x = particlesPxPos[0].min() - (w // 2)\n",
    "    max_x = particlesPxPos[0].max() + (w // 2)\n",
    "    min_y = particlesPxPos[1].min() - (h // 2)\n",
    "    max_y = particlesPxPos[1].max() + (h // 2)\n",
    "    \n",
    "    reduced_mask = (\n",
    "                    (keypointBase_np[:, 0] <= max_x) & (keypointBase_np[:, 0] >= min_x) &\n",
    "                    (keypointBase_np[:, 1] <= max_y) & (keypointBase_np[:, 1] >= min_y)\n",
    "                    )\n",
    "    \n",
    "    # aug_keypoints_np = [kp for kp, mask in zip(keypoints_np, reduced_mask) if mask]\n",
    "    reduced_keypoints = keypointBase_np[reduced_mask]\n",
    "    if detector == 'ORB':\n",
    "        reduced_descriptors = featuresBase[reduced_mask]\n",
    "    elif detector == 'SP':\n",
    "        keypoints, keypoint_scores, descriptors, image_size = featuresBase[\"keypoints\"][:,reduced_mask,:] , featuresBase[\"keypoint_scores\"][:,reduced_mask], featuresBase[\"descriptors\"][:,reduced_mask,:] , featuresBase[\"image_size\"]\n",
    "        image_size = torch.from_numpy(np.array([w,h])[np.newaxis, : ]).to(device)\n",
    "        # reduced_descriptors = {\"keypoints\" : keypoints, \"keypoint_scores\" : keypoint_scores, \"descriptors\" : descriptors , \"image_size\" : image_size}\n",
    "        \n",
    "        # scales, oris =  featuresBase[\"scales\"][:,reduced_mask] , featuresBase[\"oris\"][:,reduced_mask]\n",
    "        reduced_descriptors = {\"keypoints\" : keypoints, \"keypoint_scores\" : keypoint_scores, \"descriptors\" : descriptors , \"image_size\" : image_size}#, \"scales\" : scales, \"oris\" : oris}\n",
    "        \n",
    "        \n",
    "    # ParticlesKeypoints      = []\n",
    "    # ParticlesDescriptors    = []\n",
    "    # ParticlesLocalKeypoints = []\n",
    "\n",
    "    # with Timer('dd'):\n",
    "    # for i in range(N):\n",
    "    # yaw = particlesYaw[i]  # rad\n",
    "    # Compute rotation matrix\n",
    "    R = np.array([\n",
    "        [ np.cos(yaw),  np.sin(yaw)],\n",
    "        [-np.sin(yaw),  np.cos(yaw)]\n",
    "    ])\n",
    "\n",
    "    # Shift keypoints to rectangle's center\n",
    "    shifted_keypoints = reduced_keypoints - particlesPxPos\n",
    "\n",
    "    # Rotate keypoints to rectangle's local frame\n",
    "    local_keypoints = np.dot(shifted_keypoints, R) \n",
    "            \n",
    "    inside_mask = (\n",
    "        (np.abs(local_keypoints[:, 0]) <= w // 2) &\n",
    "        (np.abs(local_keypoints[:, 1]) <= h // 2)\n",
    "    )\n",
    "    \n",
    "    # Mask inside features\n",
    "    particle_keypoint        = reduced_keypoints[inside_mask]\n",
    "    particle_local_keyppoint = local_keypoints[inside_mask]  + np.array([ w // 2, h // 2])    # Particles Local Keypoints (relative keypoints  to uppler left corner of particles px)\n",
    "    if detector == 'ORB':\n",
    "        particle_descriptor = reduced_descriptors[inside_mask]\n",
    "    elif detector == 'SP':\n",
    "        keypoints, keypoint_scores, descriptors, image_size = reduced_descriptors[\"keypoints\"][:,inside_mask,:] , reduced_descriptors[\"keypoint_scores\"][:,inside_mask], reduced_descriptors[\"descriptors\"][:,inside_mask,:] , reduced_descriptors[\"image_size\"]\n",
    "        image_size = torch.from_numpy(np.array([w,h])[np.newaxis, : ]).to(device)\n",
    "        # particle_descriptor = {\"keypoints\" : keypoints, \"keypoint_scores\" : keypoint_scores, \"descriptors\" : descriptors , \"image_size\" : image_size}\n",
    "\n",
    "        # scales, oris =  reduced_descriptors[\"scales\"][:,inside_mask] , reduced_descriptors[\"oris\"][:,inside_mask]\n",
    "        particle_descriptor = {\"keypoints\" : keypoints, \"keypoint_scores\" : keypoint_scores, \"descriptors\" : descriptors , \"image_size\" : image_size}#, \"scales\" : scales, \"oris\" : oris}\n",
    "        \n",
    "    # ParticlesKeypoints.append(particle_keypoint)\n",
    "    # ParticlesDescriptors.append(particle_descriptor)\n",
    "    # ParticlesLocalKeypoints.append(particle_local_keyppoint)\n",
    "                \n",
    "    return particle_local_keyppoint,particle_keypoint,particle_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats0['descriptors'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 39286)\n"
     ]
    }
   ],
   "source": [
    "b_concat = np.hstack((b, b))\n",
    "print(np.atleast_2d(b_concat).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "torch.from_numpy(np.array([1,2])[np.newaxis, : ]).to('cuda').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difficult example\n",
    "For pairs with significant viewpoint- and illumination changes, LightGlue can exclude a lot of points early in the matching process (red points), which significantly reduces the inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  82, 1884],\n",
      "        [ 106, 1124],\n",
      "        [ 111,  332],\n",
      "        [ 125, 1855],\n",
      "        [ 134,  498],\n",
      "        [ 169, 1511],\n",
      "        [ 191, 1857],\n",
      "        [ 197, 1968],\n",
      "        [ 236,  492],\n",
      "        [ 272,  438],\n",
      "        [ 285,  826],\n",
      "        [ 287,  436],\n",
      "        [ 295,  144],\n",
      "        [ 296, 1200],\n",
      "        [ 300,   22],\n",
      "        [ 306,   97],\n",
      "        [ 307,  371],\n",
      "        [ 311, 1163],\n",
      "        [ 316,  217],\n",
      "        [ 319,  783],\n",
      "        [ 324, 1654],\n",
      "        [ 325,  307],\n",
      "        [ 326,  183],\n",
      "        [ 330,   83],\n",
      "        [ 334,  443],\n",
      "        [ 335, 1171],\n",
      "        [ 337,  473],\n",
      "        [ 339,  846],\n",
      "        [ 345,  559],\n",
      "        [ 350,  541],\n",
      "        [ 353,  655],\n",
      "        [ 357, 1585],\n",
      "        [ 358,  218],\n",
      "        [ 364,  901],\n",
      "        [ 365, 1667],\n",
      "        [ 370, 1800],\n",
      "        [ 372,  970],\n",
      "        [ 373, 1938],\n",
      "        [ 374, 1549],\n",
      "        [ 375,  393],\n",
      "        [ 376, 1177],\n",
      "        [ 378, 1642],\n",
      "        [ 390,  975],\n",
      "        [ 391,  930],\n",
      "        [ 397,  329],\n",
      "        [ 402,  585],\n",
      "        [ 407, 1715],\n",
      "        [ 408, 1175],\n",
      "        [ 412, 1734],\n",
      "        [ 414,  398],\n",
      "        [ 415,  164],\n",
      "        [ 422, 1226],\n",
      "        [ 426,  360],\n",
      "        [ 430, 1471],\n",
      "        [ 435, 1820],\n",
      "        [ 439, 1887],\n",
      "        [ 447, 1562],\n",
      "        [ 449, 1486],\n",
      "        [ 452,  743],\n",
      "        [ 453, 1824],\n",
      "        [ 460,  838],\n",
      "        [ 462,  310],\n",
      "        [ 468,  507],\n",
      "        [ 472,    5],\n",
      "        [ 474, 1491],\n",
      "        [ 482,  528],\n",
      "        [ 483,   88],\n",
      "        [ 500,   30],\n",
      "        [ 502, 1589],\n",
      "        [ 504,   41],\n",
      "        [ 506,  256],\n",
      "        [ 507, 1526],\n",
      "        [ 511,    4],\n",
      "        [ 515,  177],\n",
      "        [ 516,  519],\n",
      "        [ 519,  875],\n",
      "        [ 520,  862],\n",
      "        [ 524,  213],\n",
      "        [ 527,   55],\n",
      "        [ 536, 1993],\n",
      "        [ 537,   25],\n",
      "        [ 538,   62],\n",
      "        [ 541,    9],\n",
      "        [ 542,  238],\n",
      "        [ 550,  769],\n",
      "        [ 554,  870],\n",
      "        [ 555, 1356],\n",
      "        [ 557,   19],\n",
      "        [ 562,  231],\n",
      "        [ 563,   59],\n",
      "        [ 565,  703],\n",
      "        [ 567,  824],\n",
      "        [ 569, 2002],\n",
      "        [ 571,  102],\n",
      "        [ 576,   40],\n",
      "        [ 579, 1749],\n",
      "        [ 583,  124],\n",
      "        [ 587,  182],\n",
      "        [ 589,  839],\n",
      "        [ 591,  110],\n",
      "        [ 593, 1759],\n",
      "        [ 595,  156],\n",
      "        [ 597,  757],\n",
      "        [ 599,   92],\n",
      "        [ 601,  685],\n",
      "        [ 603, 1735],\n",
      "        [ 604,  472],\n",
      "        [ 607,  600],\n",
      "        [ 609,  516],\n",
      "        [ 611,  863],\n",
      "        [ 613,   99],\n",
      "        [ 614,  672],\n",
      "        [ 617,  429],\n",
      "        [ 619, 1575],\n",
      "        [ 621,  932],\n",
      "        [ 624,  440],\n",
      "        [ 625,  264],\n",
      "        [ 631, 1259],\n",
      "        [ 632, 1960],\n",
      "        [ 637,   17],\n",
      "        [ 639,  755],\n",
      "        [ 644, 1203],\n",
      "        [ 646, 1021],\n",
      "        [ 648, 1474],\n",
      "        [ 649,  785],\n",
      "        [ 651,  529],\n",
      "        [ 655,  351],\n",
      "        [ 659,   64],\n",
      "        [ 660, 1818],\n",
      "        [ 661, 1044],\n",
      "        [ 669, 1339],\n",
      "        [ 675,   32],\n",
      "        [ 676,  961],\n",
      "        [ 678,  331],\n",
      "        [ 679, 1321],\n",
      "        [ 686,  162],\n",
      "        [ 690, 1364],\n",
      "        [ 697,  495],\n",
      "        [ 699,   65],\n",
      "        [ 700, 1790],\n",
      "        [ 701, 1712],\n",
      "        [ 702,  715],\n",
      "        [ 703, 1034],\n",
      "        [ 707,  784],\n",
      "        [ 710, 2000],\n",
      "        [ 712,  414],\n",
      "        [ 715,  408],\n",
      "        [ 719,  201],\n",
      "        [ 720, 1996],\n",
      "        [ 725, 1052],\n",
      "        [ 729,  140],\n",
      "        [ 733, 2006],\n",
      "        [ 739, 1862],\n",
      "        [ 744,  317],\n",
      "        [ 745, 1577],\n",
      "        [ 751,  320],\n",
      "        [ 757, 1677],\n",
      "        [ 758,  415],\n",
      "        [ 759,   15],\n",
      "        [ 760, 1733],\n",
      "        [ 764,  760],\n",
      "        [ 767,  454],\n",
      "        [ 770,  205],\n",
      "        [ 772, 1212],\n",
      "        [ 773,  352],\n",
      "        [ 778, 1348],\n",
      "        [ 779,  808],\n",
      "        [ 784,  991],\n",
      "        [ 786,  480],\n",
      "        [ 790, 1780],\n",
      "        [ 794,  509],\n",
      "        [ 798,  535],\n",
      "        [ 802,  467],\n",
      "        [ 811,  588],\n",
      "        [ 812, 1007],\n",
      "        [ 820,  416],\n",
      "        [ 836,   68],\n",
      "        [ 837,  920],\n",
      "        [ 843,  827],\n",
      "        [ 844, 1531],\n",
      "        [ 848, 1198],\n",
      "        [ 853, 1074],\n",
      "        [ 855, 1379],\n",
      "        [ 858,  802],\n",
      "        [ 859,  518],\n",
      "        [ 864, 1827],\n",
      "        [ 867, 1056],\n",
      "        [ 876,  911],\n",
      "        [ 879, 2016],\n",
      "        [ 887,   96],\n",
      "        [ 892,  741],\n",
      "        [ 893,  378],\n",
      "        [ 907,  105],\n",
      "        [ 908,  593],\n",
      "        [ 909,  639],\n",
      "        [ 915,  203],\n",
      "        [ 921,  674],\n",
      "        [ 922,  425],\n",
      "        [ 925,  326],\n",
      "        [ 928, 1557],\n",
      "        [ 933, 1666],\n",
      "        [ 938,  817],\n",
      "        [ 949,  324],\n",
      "        [ 966,   36],\n",
      "        [ 967,  384],\n",
      "        [ 972, 1729],\n",
      "        [ 978, 1469],\n",
      "        [ 996, 1826],\n",
      "        [1012,  547],\n",
      "        [1033, 1629],\n",
      "        [1040,  843],\n",
      "        [1041, 1891],\n",
      "        [1052,  155],\n",
      "        [1076, 1141],\n",
      "        [1078, 1327],\n",
      "        [1094, 2011],\n",
      "        [1098, 1129],\n",
      "        [1103, 1590],\n",
      "        [1137, 1063],\n",
      "        [1141, 1998],\n",
      "        [1150,  180],\n",
      "        [1155,  214],\n",
      "        [1184,  285],\n",
      "        [1187,  976],\n",
      "        [1200,  729],\n",
      "        [1212, 1110],\n",
      "        [1213,  411],\n",
      "        [1214,  100],\n",
      "        [1216,  380],\n",
      "        [1222,  775],\n",
      "        [1230,  643],\n",
      "        [1244, 1440],\n",
      "        [1245, 1664],\n",
      "        [1253, 2043],\n",
      "        [1268, 1105],\n",
      "        [1281, 1084],\n",
      "        [1285, 1101],\n",
      "        [1303, 1049],\n",
      "        [1311,   61],\n",
      "        [1332,  829],\n",
      "        [1343,  316],\n",
      "        [1347, 1359],\n",
      "        [1392,  868],\n",
      "        [1402,  170],\n",
      "        [1503,  311]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 11\u001b[0m\n\u001b[0;32m      6\u001b[0m matches01 \u001b[38;5;241m=\u001b[39m matcher({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage0\u001b[39m\u001b[38;5;124m\"\u001b[39m: feats0, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage1\u001b[39m\u001b[38;5;124m\"\u001b[39m: feats1})\n\u001b[0;32m      8\u001b[0m feats0, feats1, matches01 \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     rbd(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [feats0, feats1, matches01]\n\u001b[0;32m     10\u001b[0m ]  \u001b[38;5;66;03m# remove batch dimension\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmatches01\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmatches\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[0;32m     14\u001b[0m kpts0, kpts1, matches \u001b[38;5;241m=\u001b[39m feats0[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m\"\u001b[39m], feats1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m\"\u001b[39m], matches01[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatches\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     15\u001b[0m m_kpts0, m_kpts1 \u001b[38;5;241m=\u001b[39m kpts0[matches[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m]], kpts1[matches[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m]]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "image0 = load_image(images / \"sacre_coeur1.jpg\")\n",
    "image1 = load_image(images / \"sacre_coeur2.jpg\")\n",
    "\n",
    "feats0 = extractor.extract(image0.to(device))\n",
    "feats1 = extractor.extract(image1.to(device))\n",
    "matches01 = matcher({\"image0\": feats0, \"image1\": feats1})\n",
    "\n",
    "feats0, feats1, matches01 = [\n",
    "    rbd(x) for x in [feats0, feats1, matches01]\n",
    "]  # remove batch dimension\n",
    "print(matches01[\"matches\"]).shape\n",
    "\n",
    "\n",
    "kpts0, kpts1, matches = feats0[\"keypoints\"], feats1[\"keypoints\"], matches01[\"matches\"]\n",
    "m_kpts0, m_kpts1 = kpts0[matches[..., 0]], kpts1[matches[..., 1]]\n",
    "\n",
    "axes = viz2d.plot_images([image0, image1])\n",
    "viz2d.plot_matches(m_kpts0, m_kpts1, color=\"lime\", lw=0.2)\n",
    "viz2d.add_text(0, f'Stop after {matches01[\"stop\"]} layers')\n",
    "\n",
    "kpc0, kpc1 = viz2d.cm_prune(matches01[\"prune0\"]), viz2d.cm_prune(matches01[\"prune1\"])\n",
    "viz2d.plot_images([image0, image1])\n",
    "viz2d.plot_keypoints([kpts0, kpts1], colors=[kpc0, kpc1], ps=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bakiMPF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
